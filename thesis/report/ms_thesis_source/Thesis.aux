\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/r>>}
\HyPL@Entry{1<</S/r>>}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{dummy.1}}
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iv}{dummy.2}}
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{dummy.4}}
\citation{suitesparse}
\citation{suitesparse}
\citation{parallelqr}
\gdef \LT@i {\LT@entry 
    {1}{52.62122pt}\LT@entry 
    {1}{302.14462pt}}
\@writefile{toc}{\contentsline {chapter}{Abbreviations}{xi}{dummy.6}}
\citation{howardmulti,thrunmulti,zhoumulti}
\HyPL@Entry{13<</S/D>>}
\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{factorgraph}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro}{{\caption@xref {fig:intro}{ on input line 17}}{2}{}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Clockwise from left: 1) A mobile robot navigating in a retail store to provide inventory solutions. They are designed to collaborate with other robots at the end of scanning an aisle. 2) Swarm of robots teaming up in a cooperative task of building lego blocks at Grasp Laboratory, University of Pennsylvania. 3) Decentralized control and planning of a multi-robot system at University of New Hampshire.\relax }}{2}{figure.caption.14}}
\@writefile{toc}{\contentsline {paragraph}{}{2}{section*.13}}
\citation{violajones}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Thesis}{3}{section.15}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.20}}
\citation{kaessisam}
\citation{kaessisam2}
\citation{zhoumulti}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Efficient Factor Graph Fusion}{4}{section.21}}
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.22}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Multi-robot Pose Graph Initialization}{4}{section.23}}
\citation{violajones}
\@writefile{toc}{\contentsline {paragraph}{}{5}{section*.24}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Multi-robot Data Association}{5}{section.25}}
\@writefile{toc}{\contentsline {paragraph}{}{6}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Organization}{6}{section.27}}
\citation{dellaertsam}
\citation{kaessisam2}
\citation{factorgraph}
\citation{suitesparse}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Multi-robot Smoothing and Mapping}{7}{chapter.28}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:two}{{2}{7}{Multi-robot Smoothing and Mapping}{chapter.28}{}}
\citation{thrunprobabilistic}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.29}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Cooperative SLAM as Probabilistic Inference}{8}{section.30}}
\newlabel{eq:jp}{{2.1}{8}{Cooperative SLAM as Probabilistic Inference}{equation.31}{}}
\citation{csmicp}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The belief network showing the cause-effect relationship of a robot navigation scenario. The robot (red) is given action inputs (plain) to navigate and collect landmark (yellow) measurements (blue). Same landmarks detected at multiple observations from a single pose indicates duplication which will be solved by data association.\relax }}{9}{figure.caption.32}}
\newlabel{fig:single_bel_net}{{2.1}{9}{The belief network showing the cause-effect relationship of a robot navigation scenario. The robot (red) is given action inputs (plain) to navigate and collect landmark (yellow) measurements (blue). Same landmarks detected at multiple observations from a single pose indicates duplication which will be solved by data association.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {paragraph}{}{9}{section*.33}}
\newlabel{eq:procmod}{{2.2}{9}{}{equation.34}{}}
\citation{Bhattacharyya}
\newlabel{eq:scanmod}{{2.3}{10}{}{equation.35}{}}
\newlabel{eq:measmod}{{2.4}{10}{}{equation.36}{}}
\newlabel{eq:procdist}{{2.5}{10}{}{equation.37}{}}
\newlabel{eq:measdist}{{2.6}{10}{}{equation.38}{}}
\citation{lsopt}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Formulating as Optimization}{11}{subsection.39}}
\newlabel{eq:lsopt}{{2.9}{12}{Formulating as Optimization}{equation.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Optimization for Multiple robots}{12}{subsection.43}}
\@writefile{toc}{\contentsline {paragraph}{}{12}{section*.45}}
\@writefile{toc}{\contentsline {paragraph}{}{12}{section*.46}}
\citation{smithekf}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A belief network showing direct and indirect encounter in a multi-robot scenario. Robot-robot encounters (direct) are shown by orange arrows and a robot-landmark-robot encounter (indirect) are shown by red arrows.\relax }}{13}{figure.caption.44}}
\newlabel{fig:multi_bel_net}{{2.2}{13}{A belief network showing direct and indirect encounter in a multi-robot scenario. Robot-robot encounters (direct) are shown by orange arrows and a robot-landmark-robot encounter (indirect) are shown by red arrows.\relax }{figure.caption.44}{}}
\newlabel{eq:multilsopt}{{2.10}{13}{}{equation.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Solving the Multi-robot Least-Square SLAM}{14}{section.48}}
\newlabel{eq:proclin}{{2.14}{14}{Solving the Multi-robot Least-Square SLAM}{equation.52}{}}
\newlabel{eq:measlin}{{2.20}{15}{Solving the Multi-robot Least-Square SLAM}{equation.58}{}}
\@writefile{toc}{\contentsline {paragraph}{}{15}{section*.61}}
\newlabel{eq:multilinlsopt}{{2.23}{15}{}{equation.62}{}}
\newlabel{eq:simplifymaho}{{2.24}{16}{}{equation.63}{}}
\newlabel{eq:multisimplifiedopt}{{2.25}{16}{}{equation.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Incremental Optimization}{16}{subsection.65}}
\citation{kaessisam2}
\citation{golubmatrixbook}
\citation{golubmatrixbook}
\citation{golubmatrixbook}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Left: Trajectory obtained by just forward integrating the odometry provided by robot. Right: Trajectory reconstructed after batch SLAM. It can be seen that the plain odometry trajectory looks same as the reconstructed trajectory at the beginning (blue triangle) but starts drifting away due to integral error.\relax }}{17}{figure.caption.66}}
\newlabel{fig:dr_vs_slam}{{2.3}{17}{Left: Trajectory obtained by just forward integrating the odometry provided by robot. Right: Trajectory reconstructed after batch SLAM. It can be seen that the plain odometry trajectory looks same as the reconstructed trajectory at the beginning (blue triangle) but starts drifting away due to integral error.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.1}Jacobian Decomposition}{17}{subsubsection.71}}
\newlabel{eqn:qrlsopti}{{2.30}{18}{Jacobian Decomposition}{equation.76}{}}
\citation{graphandla}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Foundations of Matrix and Graphs}{19}{section.77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Graph Theory Terminology}{19}{subsection.78}}
\@writefile{toc}{\contentsline {paragraph}{Glossary of Terms:}{19}{section*.79}}
\citation{factorgraph}
\citation{kaessbayestree}
\citation{oldchina}
\citation{gauss}
\citation{heggernesordering}
\citation{kaessbayestree}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Factor Graph and Bayes Tree}{21}{subsection.80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.1}Factor Graph}{21}{subsubsection.81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.2}Bayes Network}{21}{subsubsection.84}}
\citation{kaessbayestree}
\citation{fgtobn}
\citation{factorgraph}
\citation{kaessbayestree}
\newlabel{sss:bayes_net_intro}{{2.3.2.2}{22}{Bayes Network}{subsubsection.84}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.3}Bayes Tree}{22}{subsubsection.85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Matrix vs. Graph}{22}{subsection.86}}
\citation{dellaertsam}
\citation{kaessbayestree}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.1}Jacobian vs. Factor Graph}{23}{subsubsection.87}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.2}Square-root Information vs. Bayes Network}{23}{subsubsection.91}}
\citation{kaessisam2}
\citation{kaessbayestree}
\citation{golubmatrixbook}
\citation{kaesscovariance}
\citation{chordalcliqueintro,kaessbayestree,fgtobn,heggernesordering,oldchina,cowellprobabilistic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.3}Square-root Information vs. Bayes Tree}{24}{subsubsection.92}}
\newlabel{sss:r_vs_bt}{{2.3.3.3}{24}{Square-root Information vs. Bayes Tree}{subsubsection.92}{}}
\citation{orderingnphard}
\citation{colamd,heggernesordering}
\citation{latrend}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Factor Graph $\rightarrow $ Bayes Network $\rightarrow $ Bayes Tree}{25}{subsection.93}}
\newlabel{ss:fg_to_bn_to_bt}{{2.3.4}{25}{Factor Graph $\rightarrow $ Bayes Network $\rightarrow $ Bayes Tree}{subsection.93}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Variable Reordering}{25}{section.96}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Eliminating a factor graph into a Bayes net. The elimination order used for converting is $O = [l_3, l_2, l_1, x_4, x_3, x_2, x_1]$. The vertex of the factor graph that is removed at every step is shown by a red dotted separator. The horizontal arrows indicate the equivalent Bayes net on removing a factor graph vertex. The vertical arrows point the progress within the factor graph and Bayes net.\relax }}{26}{figure.caption.94}}
\newlabel{fig:fg_to_bn}{{2.4}{26}{Eliminating a factor graph into a Bayes net. The elimination order used for converting is $O = [l_3, l_2, l_1, x_4, x_3, x_2, x_1]$. The vertex of the factor graph that is removed at every step is shown by a red dotted separator. The horizontal arrows indicate the equivalent Bayes net on removing a factor graph vertex. The vertical arrows point the progress within the factor graph and Bayes net.\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Converting a Bayes net into a Bayes tree by clique factorization. The bubble around the node(s) in the Bayes net indicate the cliques found at every iteration based on reverse elimination ordering $x_1, x_2, x_3, x_4, l_1, l_2, l_3$. The color of the bubble denotes the node being added to the same colored clique.\relax }}{27}{figure.caption.95}}
\newlabel{fig:bn_to_bt}{{2.5}{27}{Converting a Bayes net into a Bayes tree by clique factorization. The bubble around the node(s) in the Bayes net indicate the cliques found at every iteration based on reverse elimination ordering $x_1, x_2, x_3, x_4, l_1, l_2, l_3$. The color of the bubble denotes the node being added to the same colored clique.\relax }{figure.caption.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Variable Ordering for QR Factorization}{28}{subsection.97}}
\newlabel{eq:Qshort}{{2.36}{28}{Variable Ordering for QR Factorization}{equation.100}{}}
\newlabel{eq:R}{{2.38}{28}{Variable Ordering for QR Factorization}{equation.102}{}}
\@writefile{toc}{\contentsline {paragraph}{}{29}{section*.103}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Left: Matrix in which the lower degree node $x_2$, $d(x_2)=1$, is ordered before the higher degree node 3, $d(x_3)=3$. The structure of square root factor $R^O$ with 11 non-zero elements.\relax }}{30}{figure.caption.104}}
\newlabel{fig:good_ordering}{{2.6}{30}{Left: Matrix in which the lower degree node $x_2$, $d(x_2)=1$, is ordered before the higher degree node 3, $d(x_3)=3$. The structure of square root factor $R^O$ with 11 non-zero elements.\relax }{figure.caption.104}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Eliminating a factor graph into a Bayes net and in turn into a Bayes tree. A good ordering has resulted in smaller cliques or reduced fill-in.\relax }}{30}{figure.caption.105}}
\newlabel{fig:good_ordering_graph}{{2.7}{30}{Eliminating a factor graph into a Bayes net and in turn into a Bayes tree. A good ordering has resulted in smaller cliques or reduced fill-in.\relax }{figure.caption.105}{}}
\@writefile{toc}{\contentsline {paragraph}{}{30}{section*.106}}
\citation{dufftrackingexpensive,georgetrackingexpensive,eisenstattrackingexpensive}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Left:Jacobian matrix with highest degree variable $x_3$ of degree $d(x_3)=3$ in the first place. Right: The square-root factor with higher fill-in when compared the ordering $O = [x_2, x_3, x_4, x_1, x_5]$. Although the increase in non-zeros is only 4, it is very large given the original fill-in and the size of the matrix.\relax }}{31}{figure.caption.107}}
\newlabel{fig:bad_ordering}{{2.8}{31}{Left:Jacobian matrix with highest degree variable $x_3$ of degree $d(x_3)=3$ in the first place. Right: The square-root factor with higher fill-in when compared the ordering $O = [x_2, x_3, x_4, x_1, x_5]$. Although the increase in non-zeros is only 4, it is very large given the original fill-in and the size of the matrix.\relax }{figure.caption.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Eliminating a factor graph into a Bayes net and in turn into a Bayes tree. Eliminating nodes of higher degree at the beginning has resulted in larger cliques or high fill-in.\relax }}{31}{figure.caption.108}}
\newlabel{fig:bad_ordering_graph}{{2.9}{31}{Eliminating a factor graph into a Bayes net and in turn into a Bayes tree. Eliminating nodes of higher degree at the beginning has resulted in larger cliques or high fill-in.\relax }{figure.caption.108}{}}
\citation{orderingnphard}
\citation{chordaloptimalordering}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Efficient Factor Graph Fusion}{33}{chapter.109}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:three}{{3}{33}{Efficient Factor Graph Fusion}{chapter.109}{}}
\citation{lumiliosfirstgraphslam}
\citation{cgdescent}
\citation{gdescent}
\citation{thrunprobabilistic}
\citation{mlrelaxation}
\citation{dellaertonlysam}
\citation{dellaertonlysam}
\citation{kaessisam}
\citation{kaessisam2}
\citation{lumiliosfirstgraphslam}
\citation{dellaertonlysam}
\citation{variableorderingslam}
\@writefile{toc}{\contentsline {paragraph}{}{34}{section*.110}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Related Work}{34}{section.111}}
\citation{kaessbayestree}
\citation{kaessisam2}
\citation{multipleisam}
\citation{csam}
\citation{tectonicsam}
\citation{gdescent}
\citation{fresetreemap}
\citation{freseclosing}
\citation{gdescent}
\citation{fresetreemap}
\citation{freseclosing}
\citation{hund}
\citation{exactordering}
\citation{orderingnphard}
\citation{markowitzelimination}
\citation{tinneyfirstordering}
\citation{rosegraph}
\citation{amd}
\citation{nesdis}
\citation{colamd}
\citation{georgeevolution}
\citation{hund}
\@writefile{toc}{\contentsline {paragraph}{}{35}{section*.112}}
\@writefile{toc}{\contentsline {paragraph}{}{35}{section*.113}}
\citation{kaessbayestree}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bayes Tree for Variable Ordering}{36}{section.114}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Bayes network and Bayes tree representations of the factor graph example used in last chapter with the elimination order $O = [l_3, l_2, l_1, x_4, x_3, x_2, x_1]$. The factorization of the factor graph joint probability density is mentioned for both the representations.\relax }}{37}{figure.caption.116}}
\newlabel{fig:bn_bt_factorization}{{3.1}{37}{Bayes network and Bayes tree representations of the factor graph example used in last chapter with the elimination order $O = [l_3, l_2, l_1, x_4, x_3, x_2, x_1]$. The factorization of the factor graph joint probability density is mentioned for both the representations.\relax }{figure.caption.116}{}}
\@writefile{toc}{\contentsline {paragraph}{}{37}{section*.117}}
\citation{kaessbayestree}
\citation{kaessisam2}
\@writefile{toc}{\contentsline {paragraph}{}{38}{section*.118}}
\@writefile{toc}{\contentsline {paragraph}{}{38}{section*.119}}
\@writefile{toc}{\contentsline {paragraph}{}{38}{section*.121}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Adding a new variable with measurement to a factor graph example described in the previous chapter. It should be noted that only a part of the Bayes tree (red filled nodes) to which the new variable is added is recovered, reordered and factorized. The unaffected cliques (purple filled nodes) are attached back to the new tree. The new variable added is shown used a red broken circle.\relax }}{39}{figure.caption.120}}
\newlabel{fig:bt_add_var}{{3.2}{39}{Adding a new variable with measurement to a factor graph example described in the previous chapter. It should be noted that only a part of the Bayes tree (red filled nodes) to which the new variable is added is recovered, reordered and factorized. The unaffected cliques (purple filled nodes) are attached back to the new tree. The new variable added is shown used a red broken circle.\relax }{figure.caption.120}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Multi-robot Pose Graph Fusion}{39}{subsection.122}}
\@writefile{toc}{\contentsline {paragraph}{}{39}{section*.123}}
\@writefile{toc}{\contentsline {paragraph}{}{40}{section*.125}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Fusing multiple Bayes trees to calculate the best estimate. Only the affected part (inside blob in row 3) of both the Bayes trees are recovered and refactorized. $O_1$, $O_2$ and $O_{fused}$ are the variable ordering of first, second and the fused factor graph.\relax }}{41}{figure.caption.124}}
\newlabel{fig:fusing_bt}{{3.3}{41}{Fusing multiple Bayes trees to calculate the best estimate. Only the affected part (inside blob in row 3) of both the Bayes trees are recovered and refactorized. $O_1$, $O_2$ and $O_{fused}$ are the variable ordering of first, second and the fused factor graph.\relax }{figure.caption.124}{}}
\citation{gtsamhandson}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Formal Verification}{42}{section.126}}
\citation{imt}
\citation{colamd}
\citation{kaessbayestree}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Ordering the Fused Graph}{44}{section.131}}
\newlabel{sec:fusion_ordering}{{3.4}{44}{Ordering the Fused Graph}{section.131}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Fusion Ordering}{45}{subsection.132}}
\@writefile{toc}{\contentsline {paragraph}{}{45}{section*.133}}
\citation{nesdis}
\citation{nesdisslam}
\citation{colamd}
\newlabel{eq:fused_ordering}{{3.6}{46}{}{equation.136}{}}
\newlabel{eq:permutation_matrix}{{3.7}{46}{}{equation.137}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Relation with Nested Dissection}{46}{subsection.138}}
\@writefile{toc}{\contentsline {paragraph}{}{47}{section*.139}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Numerical Stabilization}{47}{subsection.140}}
\@writefile{toc}{\contentsline {paragraph}{}{47}{section*.141}}
\citation{matching}
\citation{matching}
\citation{colamd}
\citation{suitesparse}
\citation{suitesparse}
\citation{suitesparse}
\citation{suitesparse}
\@writefile{toc}{\contentsline {paragraph}{}{48}{section*.142}}
\@writefile{toc}{\contentsline {paragraph}{}{48}{section*.143}}
\@writefile{toc}{\contentsline {paragraph}{}{48}{section*.144}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experimental Results}{48}{section.145}}
\citation{parallelqr}
\citation{parallelqr}
\citation{parallelqr}
\@writefile{toc}{\contentsline {paragraph}{}{49}{section*.148}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Comparison of number of non-zeros in the square root factor from QR factorization between the COLAMD ordering (second column), proposed ordering (third column) and the order in which the variables are retrieved (third column). The first column is the graph representation of the matrices in their lowest energy state \cite  {suitesparse}.\relax }}{50}{figure.caption.146}}
\newlabel{fig:ordering_comparison_1}{{3.4}{50}{Comparison of number of non-zeros in the square root factor from QR factorization between the COLAMD ordering (second column), proposed ordering (third column) and the order in which the variables are retrieved (third column). The first column is the graph representation of the matrices in their lowest energy state \cite {suitesparse}.\relax }{figure.caption.146}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of number of non-zeros in the square root factor from QR factorization between the COLAMD ordering (second column), proposed ordering (third column) and the order in which the variables are retrieved (third column). The first column is the graph representation of the matrices in their lowest energy state \cite  {suitesparse}.\relax }}{51}{figure.caption.147}}
\newlabel{fig:ordering_comparison_2}{{3.5}{51}{Comparison of number of non-zeros in the square root factor from QR factorization between the COLAMD ordering (second column), proposed ordering (third column) and the order in which the variables are retrieved (third column). The first column is the graph representation of the matrices in their lowest energy state \cite {suitesparse}.\relax }{figure.caption.147}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Comparison among different ordering schemes on the time taken to compute the square root factor $R$. It can be seen that the proposed fusion ordering takes less time than the COLAMD ordering as fusion ordering leaves the matrix in a state suitable for parallel QR decomposition \cite  {parallelqr}.\relax }}{52}{figure.caption.149}}
\newlabel{fig:factorization_time}{{3.6}{52}{Comparison among different ordering schemes on the time taken to compute the square root factor $R$. It can be seen that the proposed fusion ordering takes less time than the COLAMD ordering as fusion ordering leaves the matrix in a state suitable for parallel QR decomposition \cite {parallelqr}.\relax }{figure.caption.149}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparison among different ordering schemes on the number of non-zero fill-in produced. It can be seen that the COLAMD produces the least and is closely followed by fusion ordering. On the other hand, the variable memory order produces huge fill-in.\relax }}{52}{figure.caption.150}}
\newlabel{fig:nnz_comparison}{{3.7}{52}{Comparison among different ordering schemes on the number of non-zero fill-in produced. It can be seen that the COLAMD produces the least and is closely followed by fusion ordering. On the other hand, the variable memory order produces huge fill-in.\relax }{figure.caption.150}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Multi-robot Relative Pose Initialization}{53}{chapter.151}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:four}{{4}{53}{Multi-robot Relative Pose Initialization}{chapter.151}{}}
\citation{violajones}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Improved Viola-Jones Object Detection for Landmark Extraction}{54}{chapter.152}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:five}{{5}{54}{Improved Viola-Jones Object Detection for Landmark Extraction}{chapter.152}{}}
\citation{violajones}
\citation{classifier4}
\citation{violajones}
\citation{classifier4}
\citation{classifier14}
\citation{classifier13}
\@writefile{toc}{\contentsline {paragraph}{}{55}{section*.153}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Related Work}{55}{section.154}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Theoretical Results}{56}{section.155}}
\@writefile{toc}{\contentsline {paragraph}{}{56}{section*.156}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Search window size and scale factor}{57}{subsection.157}}
\newlabel{eq1}{{5.1}{57}{Search window size and scale factor}{equation.158}{}}
\newlabel{eq2}{{5.2}{57}{Search window size and scale factor}{equation.159}{}}
\@writefile{toc}{\contentsline {paragraph}{}{57}{section*.160}}
\@writefile{toc}{\contentsline {paragraph}{}{57}{section*.161}}
\newlabel{eq3}{{5.3}{57}{}{equation.162}{}}
\newlabel{eq4}{{5.4}{58}{}{equation.163}{}}
\@writefile{toc}{\contentsline {paragraph}{}{58}{section*.164}}
\newlabel{eq5}{{5.5}{58}{}{equation.165}{}}
\newlabel{eq6}{{5.6}{58}{}{equation.166}{}}
\@writefile{toc}{\contentsline {paragraph}{}{58}{section*.167}}
\newlabel{eq7}{{5.7}{58}{}{equation.168}{}}
\newlabel{eq8}{{5.8}{58}{}{equation.169}{}}
\newlabel{eq9}{{5.9}{58}{}{equation.170}{}}
\@writefile{toc}{\contentsline {paragraph}{}{58}{section*.171}}
\citation{classifier1}
\citation{classifier2}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Minimum number of neighbors}{59}{subsection.172}}
\@writefile{toc}{\contentsline {paragraph}{}{59}{section*.173}}
\citation{violajones}
\citation{classifier4}
\@writefile{toc}{\contentsline {paragraph}{}{60}{section*.174}}
\@writefile{toc}{\contentsline {paragraph}{}{60}{section*.175}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Depth of the decision tree and target image resolution}{60}{subsection.176}}
\citation{classifier3}
\@writefile{toc}{\contentsline {paragraph}{}{61}{section*.177}}
\newlabel{eq10}{{5.10}{61}{}{equation.178}{}}
\newlabel{eq11}{{5.11}{61}{}{equation.179}{}}
\citation{classifier6,classifier7}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experiments run}{62}{section.180}}
\@writefile{toc}{\contentsline {paragraph}{}{62}{section*.181}}
\newlabel{eq12}{{5.12}{63}{}{equation.182}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison of ratio of maximum and minimum search window size with time taken taken for detection. It is clear from the shape of curve (blue) connecting the time taken for every search size that it varies logarithmically. The ideal curve (green) is just shown as a reference of a log curve and is not metrically accurate because we are trying to explain the theoretical complexity.\relax }}{63}{figure.caption.183}}
\newlabel{one}{{5.1}{63}{Comparison of ratio of maximum and minimum search window size with time taken taken for detection. It is clear from the shape of curve (blue) connecting the time taken for every search size that it varies logarithmically. The ideal curve (green) is just shown as a reference of a log curve and is not metrically accurate because we are trying to explain the theoretical complexity.\relax }{figure.caption.183}{}}
\@writefile{toc}{\contentsline {paragraph}{}{63}{section*.184}}
\@writefile{toc}{\contentsline {paragraph}{}{64}{section*.185}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of scale factor with time taken taken for detection. It is clear from the shape of curve (blue) connecting the time taken for every scale factor that it varies \textit  {inverse} logarithmically. The ideal curve (green) is just shown as a reference of a inverse log curve and is not metrically accurate because we are trying to explain the theoretical complexity.\relax }}{65}{figure.caption.186}}
\newlabel{two}{{5.2}{65}{Comparison of scale factor with time taken taken for detection. It is clear from the shape of curve (blue) connecting the time taken for every scale factor that it varies \textit {inverse} logarithmically. The ideal curve (green) is just shown as a reference of a inverse log curve and is not metrically accurate because we are trying to explain the theoretical complexity.\relax }{figure.caption.186}{}}
\citation{radish}
\citation{csmicp}
\citation{radish}
\citation{gtsamhandson}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experiments and Conclusions}{66}{chapter.187}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}AP Hill Multi-robot Dataset}{66}{section.188}}
\newlabel{chap:five}{{6.1}{67}{AP Hill Multi-robot Dataset}{section.188}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Tuning the Sensor Models and GTSAM Optimizer}{67}{subsection.189}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces My caption\relax }}{68}{table.caption.191}}
\newlabel{tabel:cov}{{6.1}{68}{My caption\relax }{table.caption.191}{}}
\@writefile{toc}{\contentsline {paragraph}{}{68}{section*.190}}
\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Sensor Models}{69}{appendix.192}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:one}{{A}{69}{Sensor Models}{appendix.192}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Velocity Motion Model}{69}{section.193}}
\citation{csmpaper}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Iterative Closest Point based Scan Matching}{70}{section.196}}
\citation{lumiliosfirstgraphslam}
\@writefile{toc}{\contentsline {paragraph}{}{71}{section*.200}}
\@writefile{toc}{\vspace  {2em}}
\bibstyle{unsrtnat}
\bibdata{Bibliography}
