\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Clockwise from left: 1) A mobile robot navigating in a retail store to provide inventory solutions. They are designed to collaborate with other robots at the end of scanning an aisle. 2) Swarm of robots teaming up in a cooperative task of building lego blocks at Grasp Laboratory, University of Pennsylvania. 3) Decentralized control and planning of a multi-robot system at University of New Hampshire.\relax }}{2}{figure.caption.14}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The belief network showing the cause-effect relationship of a robot navigation scenario. The robot (red) is given action inputs (plain) to navigate and collect landmark (yellow) measurements (blue). Same landmarks detected at multiple observations from a single pose indicates duplication which will be solved by data association.\relax }}{9}{figure.caption.32}
\contentsline {figure}{\numberline {2.2}{\ignorespaces A belief network showing direct and indirect encounter in a multi-robot scenario. Robot-robot encounters (direct) are shown by orange arrows and a robot-landmark-robot encounter (indirect) are shown by red arrows.\relax }}{13}{figure.caption.44}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Left: Trajectory obtained by just forward integrating the odometry provided by robot. Right: Trajectory reconstructed after batch SLAM. It can be seen that the plain odometry trajectory looks same as the reconstructed trajectory at the beginning (blue triangle) but starts drifting away due to integral error.\relax }}{17}{figure.caption.66}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Eliminating a factor graph into a Bayes net. The elimination order used for converting is $O = [l_3, l_2, l_1, x_4, x_3, x_2, x_1]$. The vertex of the factor graph that is removed at every step is shown by a red dotted separator. The horizontal arrows indicate the equivalent Bayes net on removing a factor graph vertex. The vertical arrows point the progress within the factor graph and Bayes net.\relax }}{26}{figure.caption.94}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Converting a Bayes net into a Bayes tree by clique factorization. The bubble around the node(s) in the Bayes net indicate the cliques found at every iteration based on reverse elimination ordering $x_1, x_2, x_3, x_4, l_1, l_2, l_3$. The color of the bubble denotes the node being added to the same colored clique.\relax }}{27}{figure.caption.95}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Left: Matrix in which the lower degree node $x_2$, $d(x_2)=1$, is ordered before the higher degree node 3, $d(x_3)=3$. The structure of square root factor $R^O$ with 11 non-zero elements.\relax }}{30}{figure.caption.104}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Eliminating a factor graph into a Bayes net and in turn into a Bayes tree. A good ordering has resulted in smaller cliques or reduced fill-in.\relax }}{30}{figure.caption.105}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Left:Jacobian matrix with highest degree variable $x_3$ of degree $d(x_3)=3$ in the first place. Right: The square-root factor with higher fill-in when compared the ordering $O = [x_2, x_3, x_4, x_1, x_5]$. Although the increase in non-zeros is only 4, it is very large given the original fill-in and the size of the matrix.\relax }}{31}{figure.caption.107}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Eliminating a factor graph into a Bayes net and in turn into a Bayes tree. Eliminating nodes of higher degree at the beginning has resulted in larger cliques or high fill-in.\relax }}{31}{figure.caption.108}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Bayes network and Bayes tree representations of the factor graph example used in last chapter with the elimination order $O = [l_3, l_2, l_1, x_4, x_3, x_2, x_1]$. The factorization of the factor graph joint probability density is mentioned for both the representations.\relax }}{37}{figure.caption.116}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Adding a new variable with measurement to a factor graph example described in the previous chapter. It should be noted that only a part of the Bayes tree (red filled nodes) to which the new variable is added is recovered, reordered and factorized. The unaffected cliques (purple filled nodes) are attached back to the new tree. The new variable added is shown used a red broken circle.\relax }}{39}{figure.caption.120}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Fusing multiple Bayes trees to calculate the best estimate. Only the affected part (inside blob in row 3) of both the Bayes trees are recovered and refactorized. $O_1$, $O_2$ and $O_{fused}$ are the variable ordering of first, second and the fused factor graph.\relax }}{41}{figure.caption.124}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Comparison of number of non-zeros in the square root factor from QR factorization between the COLAMD ordering (second column), proposed ordering (third column) and the order in which the variables are retrieved (third column). The first column is the graph representation of the matrices in their lowest energy state \cite {suitesparse}.\relax }}{50}{figure.caption.146}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of number of non-zeros in the square root factor from QR factorization between the COLAMD ordering (second column), proposed ordering (third column) and the order in which the variables are retrieved (third column). The first column is the graph representation of the matrices in their lowest energy state \cite {suitesparse}.\relax }}{51}{figure.caption.147}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Comparison among different ordering schemes on the time taken to compute the square root factor $R$. It can be seen that the proposed fusion ordering takes less time than the COLAMD ordering as fusion ordering leaves the matrix in a state suitable for parallel QR decomposition \cite {parallelqr}.\relax }}{52}{figure.caption.149}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparison among different ordering schemes on the number of non-zero fill-in produced. It can be seen that the COLAMD produces the least and is closely followed by fusion ordering. On the other hand, the variable memory order produces huge fill-in.\relax }}{52}{figure.caption.150}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison of ratio of maximum and minimum search window size with time taken taken for detection. It is clear from the shape of curve (blue) connecting the time taken for every search size that it varies logarithmically. The ideal curve (green) is just shown as a reference of a log curve and is not metrically accurate because we are trying to explain the theoretical complexity.\relax }}{63}{figure.caption.183}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of scale factor with time taken taken for detection. It is clear from the shape of curve (blue) connecting the time taken for every scale factor that it varies \textit {inverse} logarithmically. The ideal curve (green) is just shown as a reference of a inverse log curve and is not metrically accurate because we are trying to explain the theoretical complexity.\relax }}{65}{figure.caption.186}
\addvspace {10\p@ }
\addvspace {10\p@ }
